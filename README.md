Natural Language Processing (NLP) is current
state of art technology which has seen major
advancements in recent years from offline
speech to text transcribing to image translation
to natural language tasks seem unbelievable.
Language models have come a long way from
traditional LSTM to new Bidirectional encoders
with relational transformers. The new
transformers-based models use two methods
namely Pretraining and Finetuning to make the
language model to adapt to every context of
natural language. Pretrained models achieve
strong improvements on tasks that involve realworld knowledge like based on context,
suggesting that large-scale language modeling
could be an implicit method to capture
knowledge. In this work, the researchers further
investigate the extent to which pretrained
models such as BERT capture knowledge and
use it effectively using a zero-shot fact
completion task and question answering tasks.
Moreover, the researchers propose a simple yet
effective task for weakly supervised pretraining
model, which explicitly forces the language
model to incorporate knowledge about realworld entities. By pretraining weakly supervised
language models to learn entity level
knowledge and storing information these
models will yield significant improvements on
the fact completion and question answering
tasks.
